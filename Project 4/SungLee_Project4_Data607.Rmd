---
title: "Data 607 Project 4"
author: "Sung Lee"
date: "4/24/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: paper
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project4 "Sung's Project 4 on RPubs")
<br>
[Rmd on Github](https://github.com/logicalschema/DATA607/blob/master/Project%204/SungLee_Project4_Data607.Rmd "Sung's Project 4 Assignment Github")


# Introduction
The purpose of this project is to get our feet wet in document classification. One application of document classification is identifying "spam" and "ham". Spam is "any kind of unwanted, unsolicited digital communication, often an email, that gets sent out in bulk."^[https://www.malwarebytes.com/spam/] Ham would be the opposite of spam and represent necessary and/or wanted digital communications. Spam can potentially contain malicious code and consume space on email servers.  

This project will employ a spam/ham dataset to train a model. This model will then be run to make predictions of a new dataset to determine spam/ham.  

These will be the libraries used for this project. I wanted to use `quanteda` package for this project. It is package for analyzing text documents and I was interested in it since Project 3. More information can be found here: https://quanteda.io/.  

````{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(readtext)
library(RColorBrewer)
library(ggplot2)
library(rvest)
library(stringr)
library(summarytools)
library(tidytext)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(caret)

# Needed for the Confusion Matrix
library(e1071)

# Random Forest method
library(randomForest)

library(tm)
```  

<div style="margin-bottom:50px;"></div>
# Import and Tidy the Data

The data to train the model is from https://spamassassin.apache.org/old/publiccorpus/. The files 20030228_spam.tar.bz2, 20030228_spam_2.tar.bz2, and 20050311_spam_2.tar.bz2 will be used. These files were uncompressed and the contents were moved to a single file. In addition, the `cmds` file located in each archive file was removed. The complete file with the combined emails is here: https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz. Another file https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz contains the emails from 20030228_easy_ham.tar.bz2, 20030228_easy_ham_2.tar.bz2, and 20030228_hard_ham.tar.bz2 with the `cmds` files removed.  

This is an import of the tar files.


```{r, message=FALSE, warning=FALSE}


# This function will strip html tags from text. This uses the rvest package
# https://stackoverflow.com/questions/17227294/removing-html-tags-from-a-string-in-r
strip_html <- function(x) {
  return(gsub("<.*?>", " ", x))
}

# Function to clean troublesome strings from text
clean_text <- function(x) {
  
  # Strip out html tags
  x <- strip_html(x)

  # Remove hostnames: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url
  # https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)
  # The tokens function has an argument 'remove_url = TRUE' but this did not work so I stripped it here
  x <- str_replace_all(x, "https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)", " ")

  # Remove Email addresses
  x <- str_replace_all(x, "\\S*@\\S*\\s?", " ")

  # Remove \n, \t, and &nbsp;
  x <- str_replace_all(x, "\n", " ")
  x <- str_replace_all(x, "\t", " ")
  x <- str_replace_all(x, "&nbsp;", " ")
  
  return(x)
  
}

# Clean Tokens: returns a DFM by tokenizing a corpus
clean_corpus <- function(x) {

# Tokenize the corpora and remove punctuation and symbols: https://quanteda.io/reference/tokens.html
temp_tokens <- tokens(x, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_separators = TRUE)

# Convert the tokens to lowercase
temp_tokens <- tokens_tolower(temp_tokens)

# Create the DFM: document-feature matrix: https://quanteda.io/reference/dfm.html
temp_dfm <- dfm(temp_tokens, remove = stopwords("english"))

return(temp_dfm)
}

# Clean Tokens: returns a DTM by tokenizing a corpus
clean_corpus_dtm <- function(x) {

corpus <- Corpus(VectorSource(x))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)

dtm <- DocumentTermMatrix(corpus)

dtm <- removeSparseTerms(dtm, 0.95)

return(dtm)
}

# The function importFiles imports a url of a zipped tar file into a variable. Creates a directory called temp
importFiles <- function(remoteURL = NULL){
  download.file(remoteURL, "temp.tgz")
  
  if (dir.exists("temp")) unlink("temp", recursive = TRUE)
  dir.create("temp")
  
  # Unzips and expands the archive of the file
  untar("temp.tgz", exdir = "temp")
  file.remove("temp.tgz")
  
  temp_data <- readtext("temp/complete/*")
  unlink("temp", recursive = TRUE)
  return(temp_data)
}

# Import the spam emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz"

raw_spam_data <- importFiles(url)
raw_spam_data <- cbind(raw_spam_data, type = "spam")

# Import the ham emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz"

raw_ham_data <- importFiles(url)
raw_ham_data <- cbind(raw_ham_data, type = "ham")

```  

Let's take a look at the data imported.

```{r}
head(raw_spam_data)
head(raw_ham_data)
```  

Before we continue, we will combine the email types and randomly choose a sample from the collection. This is also necessary for the classifier that will be used later.^[https://tutorials.quanteda.io/machine-learning/nb/]

```{r}

# Create a corpus for all the documents
combined <- rbind(raw_spam_data, raw_ham_data)
combined$text <- clean_text(combined$text) # Cleans the text



# Create a sample of 3000 documents
set.seed(2543)
sample <- combined[sample(nrow(combined), 3000), ] # Sample without replacement
sample_raw_spam_data <- subset(sample, type == "spam")
sample_raw_ham_data <- subset(sample, type == "ham")


# Test
test_raw_data <- setdiff(combined, sample)


```  




There is not a consistent format for each of the files from SpamAssassin. There are different headers for the emails in different orders across the collection. In one email, the `Content-Type` might be declared and in another the email client used would be listed. Let's convert documents we have to corpora.  

```{r}

# Store the variables as corpora
spam_corpus <- corpus(sample_raw_spam_data)
ham_corpus <- corpus(sample_raw_ham_data)
training_corpus <- corpus(sample)
testing_corpus <- corpus(test_raw_data)



```  

Here are snippets from each of the corpora.

```{r}
texts(spam_corpus)[10]
texts(ham_corpus)[10]
```  

The files are imported into the corpus format but we need to do some tidying.


First, let's clean text in our data and convert to DFMs.

```{r convert_corpus_dfm, warning=FALSE}

# Used for the Word Cloud
spam_dfm <- clean_corpus(spam_corpus)
ham_dfm <- clean_corpus(ham_corpus)


# Used for the Bayes Classifier
training_dfm <- clean_corpus(training_corpus)
testing_dfm <- clean_corpus(testing_corpus)

# Used for the Random Forest Classifier
training_dtm <- clean_corpus_dtm(sample$text)
testing_dtm <- clean_corpus_dtm(test_raw_data$text)


```


<div style="margin-bottom:50px;"></div>
# Analysis {.tabset}  

This section will go analyze the data we have imported and tidied.

<div style="margin-bottom:50px;"></div>
## Bayes Theorem

This page https://www.r-bloggers.com/text-message-classification/ has code for using a Naïve Bayes classifier for classifying messages in addition to the Quanteda page linked earlier. We will train the model and look at a summary of the model. This employs both the `caret` and `quanteda` packages.

```{r Bayes}

# Train the Bayes classifier
tmodel_nb <- textmodel_nb(training_dfm, training_dfm$type)

summary(tmodel_nb)

```  

"Naïve Bayes can only take features into consideration that occur both in the training set and the test set."^[https://tutorials.quanteda.io/machine-learning/nb/] The function `dfm_match()` can use the `training_dfm` as a pattern. The cross-table matrix shows how the classifier did.

```{r Test_inspection}
matching_dfm <- dfm_match(testing_dfm, features = featnames(training_dfm))

# Evaluating how the classifier did
actual_class <- matching_dfm$type
predicted_class <- predict(tmodel_nb, newdata = matching_dfm)
tab_class <- table(actual_class, predicted_class)
tab_class
```  

For further analysis, we can construct a Confusion Matrix^[https://en.wikipedia.org/wiki/Confusion_matrix] using the `caret` package. A Confusion Matrix is basically an error matrix.

```{r confusionmatrix}
confusionMatrix(tab_class, mode = "everything")
```

<div style="margin-bottom:50px;"></div>
## Random Forest 

John DeBlase's Rpub https://rpubs.com/bsnacks000/125281 and  Chris Marshall's article^[https://towardsdatascience.com/random-forest-text-classification-trump-v-obama-c09f947173dc] were the basis for this. We use the sample data from before to develop a Random Forest classifier. For more informatio about the Random Forest, you can see this article: https://en.wikipedia.org/wiki/Random_forest. 

```{r algorithms}

# Convert the training_dtm and testing_dtm to data frames
rf_df <- as.data.frame(as.matrix(training_dtm))
colnames(rf_df) <- make.names(colnames(rf_df))

rf_test <- as.data.frame(as.matrix(testing_dtm))
colnames(rf_test) <- make.names(colnames(rf_test))


# Add the type variable to the data frame
rf_df$rf_variable_spam <- sample$type
rf_test$rf_variable_spam <- test_raw_data$type


# Train the Random Forest Model
rf_model <- randomForest(rf_variable_spam~., data = rf_df)
summary(rf_model)

# Evaluating how the classifier did
prediction_rf_class <- predict(rf_model, type="prob")[,2] 


# Evaluate the performance of the random forest model on the training set
rf_tab <- table(rf_df$rf_variable_spam, prediction_rf_class > 0.5)
colnames(rf_tab) <- c("spam", "ham")
rf_tab

```  

Let's use the model on the testing data.

```{r rf_testing}
rf_model <- randomForest(rf_variable_spam~., data = rf_test)
prediction_rf_test <- predict(rf_model, newdata=rf_test, type="prob")[,2] 
rf_tab_test <- table(rf_test$rf_variable_spam, prediction_rf_test > 0.5)
colnames(rf_tab_test) <- c("spam", "ham")
rf_tab_test


```



<div style="margin-bottom:50px;"></div>
## Word Clouds

Below is a look at the word clouds for the spam and ham data. Notice how `localhost` and domain names appear. In spam classifying, domains can be filtered out.

```{r}
# Create a word cloud: https://quanteda.io/reference/textplot_wordcloud.html
spam.colors <- brewer.pal(9, "Paired")  

textplot_wordcloud(spam_dfm, min_count = 200, color = spam.colors)  
title("Spam Wordcloud", col.main = "black")
```  


```{r}
# Create a word cloud: https://quanteda.io/reference/textplot_wordcloud.html
ham.colors <- brewer.pal(9, "Paired")  

textplot_wordcloud(ham_dfm, min_count = 200, color = ham.colors)  
title("Ham Wordcloud", col.main = "black")
```


# Conclusion

Witha  cursory entry into classifiers, I found that it was easier to use the `quanteda` and `caret` packages to use the Naïve Bayes classifier for spam. However, the Random Forest Classifier can be tweaked to obtain a higher accuracy. Sophisticated spam filters can be employed to examine the source address, MAC addresses, and domains to quickly remove spam.

