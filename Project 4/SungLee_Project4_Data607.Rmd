---
title: "Data 607 Project 4"
author: "Sung Lee"
date: "4/24/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: paper
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project4 "Sung's Project 4 on RPubs")
<br>
[Rmd on Github](https://github.com/logicalschema/DATA607/blob/master/Project%204/SungLee_Project4_Data607.Rmd "Sung's Project 4 Assignment Github")


# Introduction
The purpose of this project is to get our feet wet in document classification. One application of document classification is identifying "spam" and "ham". Spam is "any kind of unwanted, unsolicited digital communication, often an email, that gets sent out in bulk."^[https://www.malwarebytes.com/spam/] Ham would be the opposite of spam and represent necessary and/or wanted digital communications. Spam can potentially contain malicious code and consume space on email servers.  

This project will employ a spam/ham dataset to train a model. This model will then be run to make predictions of a new dataset to determine spam/ham.  

These will be the libraries used for this project. I wanted to use `quanteda` package for this project. It is package for analyzing text documents and I was interested in it since Project 3. More information can be found here: https://quanteda.io/.  

````{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(readtext)
library(RColorBrewer)
library(ggplot2)
library(rvest)
library(stringr)
library(summarytools)
library(tidytext)
library(tidyverse)
library(quanteda)
```  

<div style="margin-bottom:50px;"></div>
# Import and Tidy the Data

The data to train the model is from https://spamassassin.apache.org/old/publiccorpus/. The files 20030228_spam.tar.bz2, 20030228_spam_2.tar.bz2, and 20050311_spam_2.tar.bz2 will be used. These files were uncompressed and the contents were moved to a single file. In addition, the `cmds` file located in each archive file was removed. The complete file with the combined emails is here: https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz. Another file https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz contains the emails from 20030228_easy_ham.tar.bz2, 20030228_easy_ham_2.tar.bz2, and 20030228_hard_ham.tar.bz2 with the `cmds` files removed.  

This is an import of the tar files.


```{r, message=FALSE, warning=FALSE}


# This function will strip html tags from text. This uses the rvest package
# https://www.rdocumentation.org/packages/rvest/versions/0.3.5/topics/html_text
strip_html <- function(x) {
    return(html_text(read_html(x)))
}

# The function importFiles imports a url of a zipped tar file into a variable. Creates a directory called temp
importFiles <- function(remoteURL = NULL){
  download.file(remoteURL, "temp.tgz")
  
  if (dir.exists("temp")) unlink("temp", recursive = TRUE)
  dir.create("temp")
  
  # Unzips and expands the archive of the file
  untar("temp.tgz", exdir = "temp")
  file.remove("temp.tgz")
  
  temp_data <- readtext("temp/complete/*")
  unlink("temp", recursive = TRUE)
  return(temp_data)
}

# Import the spam emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz"

raw_spam_data <- importFiles(url)


# Import the ham emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz"

raw_ham_data <- importFiles(url)

```  

Let's take a look at the data imported.

```{r}
head(raw_spam_data)
head(raw_ham_data)
```

There is not a consistent format for each of the files from SpamAssassin. There are different headers for the emails in different orders across the collection. In one email, the `Content-Type` might be declared and in another the email client used would be listed. Let's convert documents we have to corpora.  

```{r}

# Strip out html tags
raw_spam_data$text <- strip_html(raw_spam_data$text)
raw_ham_data$text <- strip_html(raw_ham_data$text)

# Remove Email addresses
raw_spam_data$text <- str_replace_all(raw_spam_data$text, "\\S*@\\S*\\s?", " ")
raw_ham_data$text <- str_replace_all(raw_ham_data$text, "\\S*@\\S*\\s?", " ")

# Remove hostnames: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url
#[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)
raw_spam_data$text <- str_replace_all(raw_spam_data$text, "[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)", " ")
raw_ham_data$text <- str_replace_all(raw_ham_data$text, "[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)", " ")


# Remove \n
raw_spam_data$text <- str_replace_all(raw_spam_data$text, "\n", " ")
raw_ham_data$text <- str_replace_all(raw_ham_data$text, "\n", " ")


# Convert the text data to the character class
raw_spam_data$text <- as.character(raw_spam_data$text)
raw_ham_data$text <- as.character(raw_ham_data$text)

# Store the variables as corpora
spam_corpus <- corpus(raw_spam_data$text)
ham_corpus <- corpus(raw_ham_data$text)

```  

Here are snippets from each of the corpora.

```{r}
texts(spam_corpus)[10]
texts(ham_corpus)[10]
```  

The files are imported into the corpus format but we need to do some tidying.


First, let's tokenize the text in our data.

```{r}

# Tokenize the corpora and remove punctuation and symbols: https://quanteda.io/reference/tokens.html
spam_tokens <- tokens(spam_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_separators = TRUE)
ham_tokens <- tokens(ham_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE)

# Convert the tokens to lowercase
spam_tokens <- tokens_tolower(spam_tokens)
ham_tokens <- tokens_tolower(ham_tokens)

# Create the DFM: document-feature matrix: https://quanteda.io/reference/dfm.html
spam_dfm <- dfm(spam_tokens, remove = stopwords("english"))
ham_dfm <- dfm(ham_tokens, remove = stopwords("english"))


# Create a word cloud: https://quanteda.io/reference/textplot_wordcloud.html
spam.col <- brewer.pal(10, "BrBG")  

textplot_wordcloud(spam_dfm, min_count = 100, color = spam.col)  
title("Spam Wordcloud", col.main = "grey14")
  
  
```






<div style="margin-bottom:50px;"></div>
# Text Mining {.tabset}

<div style="margin-bottom:50px;"></div>
## Method 1


<div style="margin-bottom:50px;"></div>
## Method 2

<div style="margin-bottom:50px;"></div>
# Conclusion



