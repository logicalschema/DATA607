---
title: "Data 607 Project 4"
author: "Sung Lee"
date: "4/24/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: paper
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project4 "Sung's Project 4 on RPubs")
<br>
[Rmd on Github](https://github.com/logicalschema/DATA607/blob/master/Project%204/SungLee_Project4_Data607.Rmd "Sung's Project 4 Assignment Github")


# Introduction
The purpose of this project is to get our feet wet in document classification. One application of document classification is identifying "spam" and "ham". Spam is "any kind of unwanted, unsolicited digital communication, often an email, that gets sent out in bulk."^[https://www.malwarebytes.com/spam/] Ham would be the opposite of spam and represent necessary and/or wanted digital communications. Spam can potentially contain malicious code and consume space on email servers.  

This project will employ a spam/ham dataset to train a model. This model will then be run to make predictions of a new dataset to determine spam/ham.  

These will be the libraries used for this project. I wanted to use `quanteda` package for this project. It is package for analyzing text documents and I was interested in it since Project 3. More information can be found here: https://quanteda.io/.  

````{r echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(readtext)
library(RColorBrewer)
library(ggplot2)
library(rvest)
library(stringr)
library(summarytools)
library(tidytext)
library(tidyverse)
library(quanteda)
```  

<div style="margin-bottom:50px;"></div>
# Import and Tidy the Data

The data to train the model is from https://spamassassin.apache.org/old/publiccorpus/. The files 20030228_spam.tar.bz2, 20030228_spam_2.tar.bz2, and 20050311_spam_2.tar.bz2 will be used. These files were uncompressed and the contents were moved to a single file. In addition, the `cmds` file located in each archive file was removed. The complete file with the combined emails is here: https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz. Another file https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz contains the emails from 20030228_easy_ham.tar.bz2, 20030228_easy_ham_2.tar.bz2, and 20030228_hard_ham.tar.bz2 with the `cmds` files removed.  

This is an import of the tar files.


```{r, message=FALSE, warning=FALSE}


# This function will strip html tags from text. This uses the rvest package
# https://stackoverflow.com/questions/17227294/removing-html-tags-from-a-string-in-r
strip_html <- function(x) {
  return(gsub("<.*?>", " ", x))
}

# Function to clean troublesome strings from text
clean_text <- function(x) {
  
  # Strip out html tags
  x <- strip_html(x)

  # Remove hostnames: https://stackoverflow.com/questions/3809401/what-is-a-good-regular-expression-to-match-a-url
  # https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)
  # The tokens function has an argument 'remove_url = TRUE' but this did not work so I stripped it here
  x <- str_replace_all(x, "https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)", " ")

  # Remove Email addresses
  x <- str_replace_all(x, "\\S*@\\S*\\s?", " ")

  # Remove \n, \t, and &nbsp;
  x <- str_replace_all(x, "\n", " ")
  x <- str_replace_all(x, "\t", " ")
  x <- str_replace_all(x, "&nbsp;", " ")
  
  return(x)
  
}



# The function importFiles imports a url of a zipped tar file into a variable. Creates a directory called temp
importFiles <- function(remoteURL = NULL){
  download.file(remoteURL, "temp.tgz")
  
  if (dir.exists("temp")) unlink("temp", recursive = TRUE)
  dir.create("temp")
  
  # Unzips and expands the archive of the file
  untar("temp.tgz", exdir = "temp")
  file.remove("temp.tgz")
  
  temp_data <- readtext("temp/complete/*")
  unlink("temp", recursive = TRUE)
  return(temp_data)
}

# Import the spam emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/spamemails.tgz"

raw_spam_data <- importFiles(url)
raw_spam_data <- cbind(raw_spam_data, type = "spam")

# Import the ham emails
url <- "https://github.com/logicalschema/DATA607/raw/master/Project%204/hamemails.tgz"

raw_ham_data <- importFiles(url)
raw_ham_data <- cbind(raw_ham_data, type = "ham")

```  

Let's take a look at the data imported.

```{r}
head(raw_spam_data)
head(raw_ham_data)
```  

Before we continue, we will combine the email types and randomly choose a sample from the collection. This is also necessary for the classifier that will be used later.^[https://tutorials.quanteda.io/machine-learning/nb/]

```{r}

# Create a corpus for all the documents
combined <- rbind(raw_spam_data, raw_ham_data)
combined$text <- clean_text(combined$text) # Cleans the text



# Create a sample of 3000 documents
set.seed(2543)
sample <- combined[sample(nrow(combined), 3000), ] # Sample without replacement
sample_raw_spam_data <- subset(sample, type == "spam")
sample_raw_ham_data <- subset(sample, type == "ham")


# Test
test <- setdiff(combined, sample)

nrow(combined)
nrow(sample)
nrow(test)

```  




There is not a consistent format for each of the files from SpamAssassin. There are different headers for the emails in different orders across the collection. In one email, the `Content-Type` might be declared and in another the email client used would be listed. Let's convert documents we have to corpora.  

```{r}

# Store the variables as corpora
spam_corpus <- corpus(sample_raw_spam_data)
ham_corpus <- corpus(sample_raw_ham_data)

```  

Here are snippets from each of the corpora.

```{r}
texts(spam_corpus)[10]
texts(ham_corpus)[10]
```  

The files are imported into the corpus format but we need to do some tidying.


First, let's tokenize the text in our data.

```{r}

# Tokenize the corpora and remove punctuation and symbols: https://quanteda.io/reference/tokens.html
spam_tokens <- tokens(spam_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_separators = TRUE)
ham_tokens <- tokens(ham_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE, remove_separators = TRUE)

# Convert the tokens to lowercase
spam_tokens <- tokens_tolower(spam_tokens)
ham_tokens <- tokens_tolower(ham_tokens)

# Create the DFM: document-feature matrix: https://quanteda.io/reference/dfm.html
spam_dfm <- dfm(spam_tokens, remove = stopwords("english"))
ham_dfm <- dfm(ham_tokens, remove = stopwords("english"))

  
```






<div style="margin-bottom:50px;"></div>
# Analysis {.tabset}

<div style="margin-bottom:50px;"></div>
## Word Clouds

Here is a look at the word clouds for the spam and ham data: 

```{r}
# Create a word cloud: https://quanteda.io/reference/textplot_wordcloud.html
spam.colors <- brewer.pal(9, "Paired")  

textplot_wordcloud(spam_dfm, min_count = 200, color = spam.colors)  
title("Spam Wordcloud", col.main = "black")
```  


```{r}
# Create a word cloud: https://quanteda.io/reference/textplot_wordcloud.html
ham.colors <- brewer.pal(9, "Paired")  

textplot_wordcloud(ham_dfm, min_count = 200, color = ham.colors)  
title("Ham Wordcloud", col.main = "black")
```

<div style="margin-bottom:50px;"></div>
## Bayes Theorem

This page https://www.r-bloggers.com/text-message-classification/ has code for using a Na√Øve Bayes classifier for classifying messages in addition to the Quanteda page linked earlier.

```{r}



```

<div style="margin-bottom:50px;"></div>
# Conclusion



