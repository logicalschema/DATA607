---
title: "Data 607 Project 3"
author: "Matt Mecoli, Philip Tanofsky, Sung Lee, Vanita Thompson"
date: "3/21/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: lumen
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project3 "Data 607 Project 3 Rpubs")
<br>
[Rmd on GitHub](https://github.com/logicalschema/DATA607/blob/master/project3/Project%203.Rmd "Team MPSV Project 3 Assignment GitHub")

<div style="margin-bottom:50px;"></div>
# Introduction  

The purpose of this assignment is to answer the question, "Which are the most valued data science skills?" Within the team we have had experiences applying for jobs, but we were each relatively new to the data science vernacular. For example, we have one seasoned programmer and a student with a background in chemical engineering. This project is our endeavor in how we answered this question. 


<div style="margin-bottom:50px;"></div>
# Approach

We had discussions about which sets of data to use. The internet is filled with a plethora of data, but few were of applicable use. LinkedIn was not readily accessible. We worked from our experiences with applying for jobs and during some brainstorming over Zoom, we came up with searching government sites for available data, Kaggle, and the possibility of web scraping Monster or Indeed. True, we did have our own presuppositions about needed skills for a data scientist, but we wanted the data to speak for itself and hopefully challenge our presuppositions.

**Jackson Pollock the Data**  

Our thinking was that if we obtain data science job listings, we would mine the text for words and phrases. In other words, if we splash the data in front of us, find some frequencies, perhaps a pattern would arise. 

![Pollock painting](https://www.christies.com/img/LotImages/2004/NYR/2004_NYR_01373_0017_000().jpg)

<br>
After googling and searching, we settled on a data set that will be reviewed in the next section. However, little did we as a group realize how much cleaning and hours would be committed to having data that was accessible and usable for the entire team.  

Continuing on, the following are the R libraries we will be using.  

```{r loadlib, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(RMySQL)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(RWeka)
library(ggplot2)
library(dplyr)
library(tidytext)
library(tidyverse)
library(knitr)
library(summarytools)
library(ggraph)
library(igraph)
library(quanteda)
library(tidyr)
library(topicmodels)

```

`RMySQL` is used for working with a MySQL database. `tm` and `RWeka` were used for text mining. Packages `SnowballC`, `wordcloud`, and `RColorBrewer` were used for creating the word cloud in R. `stringr`, `dplyr`, `tidytext`, and `tidyverse` were used for text manipulations. `ggplot2`, `ggraph`, `igraph` were used for graphs and `knitr` was used for making some tables. `topicmodels` and `quanteda` were used to derive text analysis from the data. `summarytools` provides a handy frequency function to find tally the occurrences in a column.


<div style="margin-bottom:50px;"></div>
# Importing

The data we ended up using is from the web site https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa. Phil was able to obtain the csv. The data was scraped using JobsPikr. The csv consists of 10,000 records and had the following as text fields: `crawl_timestamp`, `url`, `job_title`, `category`, `company_name`, `city`, `state`, `country`, `inferred_city`, `inferred_state`, `inferred_country`, `post_date`, `job_description`, `job_type`, `salary_offered`, `job_board`, `geo`, `cursor`, `contact_email`, `contact_phone_number`, `uniq_id`, and `html_job_desc`.  

This is the original csv with field names.

![CSV Fieldnames](https://github.com/logicalschema/DATA607/raw/master/project3/images/data_original_csv.png)

Inital import of the csv file.

```{r, warning=FALSE}

# Using read_csv as it allows for on the fly decompression of zip csv files
jobs_df <- read_csv("https://github.com/logicalschema/DATA607/raw/master/project3/data/data_scientist_united_states_job_postings_jobspikr.csv.gz")

# Look that the first group of imported rows
head(jobs_df)

jobs_desc <- tibble(id = jobs_df$uniq_id, 
                        desc = jobs_df$job_description)

jobs_desc %>% 
  select(desc) %>% 
  sample_n(5)

jobs_cat <- tibble(id = jobs_df$uniq_id, 
                        category = jobs_df$category)

jobs_cat <- jobs_cat %>% filter(jobs_cat$category != "") %>% print(n=10)

```

`job_description` is the most important field in the dataset for the data mining exercise. The field contains the complete write-up posted for the job listing. The `job `job_category` provides some context of the job listing and will be used to capture important words per category.  


<br>
Initial problems and solutions:  

1. The csv file is 48.5 MB and the character set had UTF-8 characters.  

    The file was zipped up to reduce download time. When the file is imported into R, we will tidy the data.

2. HTML code was left in the `job_description` field. Evidence of the HTML scrape of the data source.  

    Invalid characters will be removed.

3. Initial load of the csv into R would take time depending upon the hardware of the group member's system.  

    With the zip file and cutting up the file into smaller pieces, this would reduce the need for additional RAM.

4. How would we convert to a normalized database?  

    The csv was converted to a MySQL script file. The header column names were encapsulated with `"` marks. Workbench was used to create a new table on the MySQL database. Subsequently, through SQL SELECT, we normalized the data.
    
    In addition, when the data was imported in a database, Sung ran Text Analytics on the `job_description` column to find key phrases. This information was used to create a new column called `keyphrases` in the normalized database.

5. Some group member's machines did not have the same amount of RAM. Vanita's laptop had 8 GB RAM and Sung's laptop was running 16 GB.  

<br>
**How did the data look in Azure?**

When the data was imported into Microsoft's cloud service, we ran some initial tests on the data to look for patterns. Using Microsoft's Text Analytics^[https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-call-api] tools the following is a word cloud^[Note the image was created with stop words on Power Bi https://github.com/logicalschema/DATA607/raw/master/project3/images/stopwords.jpg] of the keyphrases that were discovered by Microsoft's AI:

![Word Cloud Courtesy of Azure](https://github.com/logicalschema/DATA607/raw/master/project3/images/powerbi_wordcloud.png)

The key phrases were stored in the database. Key phrases were extrapolated from the job descriptions for each of the jobs listed. Because the use of Microsoft's service was limited, we found the need for persistent storage of the results. Sung had used up his $200 credit for one of his subscriptions within a week of testing. In working with the Azure Text Analytics API, there is an option to send data via JSON to have it analyzed.  



<br>
<div style="margin-bottom 25px;"></div>
## Excursus {.tabset}

This is a brief description of some of the steps taken to help alleviate the problems of importing the information for use in our project.



<div style="margin-bottom 25px;"></div>
### Infrastructure

**Azure, MySQL, and Cognitive Text Analytics**
Sung wanted to experiment with cloud computing and so he used his SPS email to create a trial account for Azure. He created a MySQL database instance to host the databases used. Each trial account is given a $200 credit. 

**MySQL database access**
This connection is made public but is restricted by IP address. If you desire access, please email one of the Team members.

*Server*: data607.mysql.database.azure.com  
*Port*: 3306  
*Username*: data607@data607  
*Password*: student#2020  

Default timeout settings for the database server had to be lengthened to enable longer processing.

<br>

This is a [link](https://github.com/logicalschema/DATA607/raw/master/project3/data/project3data607.sql.gz) to the `mysqldump` of the database.  

***Diagram of Database***
![E-R Digram](https://raw.githubusercontent.com/logicalschema/DATA607/master/project3/images/database.png)




### Tools

**Microsoft's Cognitive Text Analytics, Power BI, Slack, GitHub, and MySQL Workbench**  

In addition to the database instance, he created, Sung created an instance for Azure's Cognitive Text Analytics to experiment to see what information Microsoft's AI can mine from our data. The tools were used to facilitate handling the data.

Power BI is a Microsoft product that is used to visualize data. It was employed to work with Microsoft's Cognitive Text Analytics to extrapolate keyphrases from the `job_descriptions` of the data and to create a simple word cloud to compare with our later analysis.

Slack and GitHub were used to collaborate. Files were exchanged via Slack. Code was edited via GitHub.

MySQL Workbench was used to connect to the MySQL database.


### Data Management

As an option to curtail the amount of time needed to process the 10,000 row csv, group members worked with a small subset of the file, tested code, and then would work with the larger data set. Some group members machines would complain about memory errors. 

Files being worked on from Github were compressed to enable quicker transport across the network.  

This is an example of a memory error on Sung's laptop when trying to text mine on the job description data:
![Memory Error](https://github.com/logicalschema/DATA607/raw/master/project3/images/memory_error.png)  

To avert this problem, Sung modified his code to use the `Corpus` object instead of `VCorpus` for collections of text documents. A `VCorpus` is described as a *volatile* corpus that is "fully kept in memory".^[https://www.rdocumentation.org/packages/tm/versions/0.7-7/topics/VCorpus] 

In addition, wherever possible, snippets of the data objects were viewed and knitted in R Markdown. So instead of viewing 10,000 rows, we did our best to display 10-20 rows of a data set.  


<div style="margin-bottom:50px;"></div>
# Tidying 

Since we have imported our data into a database, we still need to clean up the job descriptions. We hope that we will be able to tidy the data such that we would be able to determine frequencies and see what picture the data paints for us.

We will now begin to query the database for the job descriptions and the key phrases that Azure extrapolated for us. We will store job descriptions in the variable `descriptions` and the key phrases in `keyphrases`. 


```{r}
#Form a connection to the MySQL database
mydb <- dbConnect(MySQL(), 
                  user='data607@data607', 
                  password='student#2020',
                  dbname='project3data607', 
                  host='data607.mysql.database.azure.com'
                  )

#Key phrases is stored in the meta table of the database
rs <- dbSendQuery(mydb, "SELECT keyphrases FROM meta")

#Remove argument for n = -1 if you don't want all the records in the db
dbRows <- dbFetch(rs, n = -1 )
#dbRows <- dbFetch(rs)
dbClearResult(rs)

#Job descriptions are stored in the job table of the database
rs <- dbSendQuery(mydb, "SELECT job_description FROM job")

#Remove argument for n = -1 if you don't want all the records in the db
descriptions <- dbFetch(rs, n = -1)
#descriptions <- dbFetch(rs)
dbClearResult(rs)

#This a sample of the Key Phrases from Azure
kable(dbRows[1:5, ], 
      caption = "Job Description Key Phrases", 
      col.names = c("Key Phrases") 
      )

```

<br>
*Azure Key Phrases*  

Let's get the unique key phrases that Azure found.  


```{r, warning=FALSE}

#Grab the unique keyphrases after some clean up: 
keyphrases <- unlist(str_split(dbRows, ","))
keyphrases <- str_replace_all(keyphrases, "^ ", "")
keyphrases <- str_replace_all(keyphrases, "[^\x20-\x7E]", "")
keyphrases <- str_replace_all(keyphrases, "\"", "")

# Remove the "c(" in the first element
keyphrases[[1]] <- keyphrases[[1]] %>% 
  str_replace("c[(]", "")

keyphrases <- as.data.frame(keyphrases)

#Some of the key phrases
keyphrases



#Frequency chart of the key phrases using the summary tools package
freq(keyphrases$keyphrases, order = "freq", style = "rmarkdown", rows = 1:20)


```

Briefly looking at the frequencies found in the key phrases, some skills started to appear such as Python (count: 6471), Computer Science (4441), and SQL (2672). However, some problems came us as the boiler plate for many job descriptions about diversity and Equal opportunity policies were appearing as evident by the frequency of such phrases as Religion (2746), Color (2675), and National Origin (2649).


<br>
*Tidy the job descriptions*  

Before we continue, we have to tidy the job descriptions. We will remove non-ASCII characters, remove some unecessary characters, stop words, make the text lowercase, and white space. Stop words are common words such as *the*, *is*, *at*, *which*, etc. This will be handy in our analysis.

```{r, warning=FALSE}


#Remove non-ASCII characters
descriptions <-  gsub("[^\x20-\x7E]", " ", descriptions)


#Convert our variable to a Corpus variable for the package tm to work on.
#A corpus is a collection of documents
descriptions <- Corpus(VectorSource(descriptions))

#Create a function to make patterns into spaces
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

descriptions <- tm_map(descriptions, toSpace, "/")
descriptions <- tm_map(descriptions, toSpace, "\n")
descriptions <- tm_map(descriptions, toSpace, "@")
descriptions <- tm_map(descriptions, toSpace, "\\|")

#Convert the text to lowercase
descriptions <- tm_map(descriptions, content_transformer(tolower))

#Remove English stop words
descriptions <- tm_map(descriptions, removeWords, stopwords("english"))

#Remove punctuation , ., etc.
descriptions <- tm_map(descriptions, removePunctuation)

#Remove whitespace
descriptions <- tm_map(descriptions, stripWhitespace)

```






<div style="margin-bottom:50px;"></div>
# Text Mining {.tabset}


After we have imported and tidied our data, we will perform tests to mine it.


<br>
<div style="margin-bottom:25px;"></div>
## Process Input Data  

Following the NASA case study example presented at https://www.tidytextmining.com/nasa.html for Term Frequency and Topic Modeling.  

From the job_description, tokenize all the words and remove "stop_words" which are common words in the English language to allow for focus on meaningful words of the job listing.

```{r unnest-job-description}
# Use tidytextâ€™s unnest_tokens() for the description field so we can do the text analysis.
# unnest_tokens() will tokenize all the words in the description field and create a tidy dataframe of the word by identifer

jobs_desc <- jobs_desc %>% 
  unnest_tokens(word, desc) %>% 
  anti_join(stop_words)

jobs_desc
```

Provide count in table form of the most common words in the job descriptions.

```{r jobs-description-common-words}
# Most common words in the description field
jobs_desc %>%
  count(word, sort = TRUE) %>% print(n=10)
```

Applying lowercase to all the words to ensure different cases of the same word aren't considered different.

```{r jobs-description-lowercase}
# lowercase all the words just to make sure there's no redundancy
jobs_desc <- jobs_desc %>% 
  mutate(word = tolower(word))
```

**Term Frequency**  

The term frequency times inverse document frequency (TF-IDF) is used to identify words that are especially important to a document within a collection of documents. The results are the most important words in the description fields as measured by TF-IDF, meaning the words are common but not too common.

1. Calculate the TF-IDF

```{r calculate-tf-idf}
# Calculating tf-idf for the description fields

desc_tf_idf <- jobs_desc %>% 
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

desc_tf_idf %>% filter(n >= 10) %>%
  arrange(-tf_idf)
```

2. Combine the data frame of the TF_IDF of the job descriptions with the job categories.

The join is performed on the unique ID as key. Joining with the categories will identify the most important words from the job descriptions per job category.

```{r td-idf-plot, fig.width=10,fig.height=40}
# Join with the category
desc_tf_idf <- full_join(desc_tf_idf, jobs_cat, by = "id")

desc_tf_idf %>% 
  filter(!near(tf, 1)) %>%
  filter(category %in% jobs_cat$category) %>%
  arrange(desc(tf_idf)) %>%
  group_by(category) %>%
  distinct(word, category, .keep_all = TRUE) %>%
  top_n(8, tf_idf) %>% 
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, tf_idf, fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 3, scales = "free") +
  coord_flip() +
  labs(title = "Highest tf-idf words in job listing description fields",
       caption = "From jobpickr dataset",
       x = NULL, y = "tf-idf")
```

The resulting plot did not prove useful for identifying skills across all the job listings for data scientist. The plot does indicate which words are more common across that specific job category. The results demonstrate that job listings by category are likely posted by the same company or same recruiter and thus the same boilerplate description is often used across many job listings.

** Topic Modeling ** 

In order to peform topic modeling, a document term matrix is required.

1. Calculate the word count by document ID. Each job description is considered a unique document by the job listing's unique ID.

```{r word-counts}
# 8.4 Topic Modeling

# Casting to a document-term matrix
word_counts <- jobs_desc %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

word_counts %>% print(n=10)
```

2. Construct the document-term matrix.

The resulting document-term matrix indicates a high level of sparsity. The non-zero entries do correspond to a certain word appearing in a particular document.

```{r construct-dtm}
# Construct DTM
desc_dtm <- word_counts %>%
  cast_dtm(id, word, n)

desc_dtm
```

3. Calculate the LDA

According to Wikipedia, "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."

```{r calculate-lda}
# Rrunning this model is time intensive
# Define there to be 16 topics.
desc_lda <- LDA(desc_dtm, k = 16, control = list(seed = 1234))
desc_lda
```

4. Tidy the resulting LDA topics.

```{r tidy-lda}
# Interpreting the data model
tidy_lda <- tidy(desc_lda)

tidy_lda
```

5. Identify the top 10 terms for each topic.

```{r top-10-tidy-lda}
# Top 10 Terms
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

6. Plot the top 10 terms for each topic.

Even though the topics are anonymous, only identified by number, the groupings of words show some similarities and differences, but do not necessarily provide much value at this point.

The topic modeling process has identified groupings of terms that we can understand as human readers of these description fields.

```{r top-terms-plot, fig.width=10,fig.height=11}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

7. Calculate gamma

Gamma will define the probability that each document belongs in each topic.

```{r lda-gamma, fig.width=10,fig.height=11}
# LDA gamma
lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma
```

8. Identify the categories associated with each topic

```{r lda-gamma-top-category}
lda_gamma <- full_join(lda_gamma, jobs_cat, by = c("document" = "id"))

lda_gamma

top_cats <- lda_gamma %>% 
  filter(gamma > 0.5) %>% 
  count(topic, category, sort = TRUE)

top_cats <- top_cats %>% filter(!is.na(category))
```

Topic 9 identifes 'business and financial operations' as the top category, and the only topic to include the term 'aws'. Topic 4, most identified with category 'Arts/Entertainment/Publishing' contains the terms 'experience' and 'content' which align with the category broadly.

```{r lda-gamma-top-category-plot, fig.width=10,fig.height=12}
# One more graph from 8.4.4
top_cats %>%
  group_by(topic) %>%
  top_n(5, n) %>%
  ungroup %>%
  mutate(category = reorder_within(category, n, topic)) %>%
  ggplot(aes(category, n, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top categories for each LDA topic",
       x = NULL, y = "Number of documents") +
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~ topic, ncol = 2, scales = "free")
```

<br>
<div style="margin-bottom:25px;"></div>
## Lexical Dispersion

Following the example at **https://www.r-bloggers.com/advancing-text-mining-with-r-and-quanteda/** and  **https://quanteda.io/articles/pkgdown/examples/plotting.html**

1. Create a corpus based on the unique ID and the job description.

```{r generate-corpus}
# Generate a corpus

uniq_jobs_df <- jobs_df %>% distinct(uniq_id, .keep_all = TRUE)

my_corpus <- corpus(uniq_jobs_df, docid_field = "uniq_id", text_field = "job_description")

mycorpus_stats <- summary(my_corpus)
```

2. Preprocess the text.

Remove numbers, remove punctuation, remove symbols, remove URLs, split hyphens. Clean for OCR.

```{r preprocess-corpus}
# Preprocess the text

# Create tokens
token <-
  tokens(
    my_corpus,
    remove_numbers  = TRUE,
    remove_punct    = TRUE,
    remove_symbols  = TRUE,
    remove_url      = TRUE,
    split_hyphens   = TRUE
  )

# Clean tokens created by OCR
token_ungd <- tokens_select(
  token,
  c("[\\d-]", "[[:punct:]]", "^.{1,2}$"),
  selection = "remove",
  valuetype = "regex",
  verbose = TRUE
)
```

3. Create a Data Frequency Matrix

Using the Quanteda library, create the data frequency matrix and filter words that appear less than 7.5% and more than 90%.

```{r create-dtm-from-corpus}
# Data frequency matrix
my_dfm <- dfm(token_ungd,
              tolower = TRUE,
              stem = TRUE,
              remove = stopwords("english")
              )

my_dfm_trim <-
  dfm_trim(
    my_dfm,
    min_docfreq = 0.075,
    # min 7.5%
    max_docfreq = 0.90,
    # max 90%
    docfreq_type = "prop"
  )

head(dfm_sort(my_dfm_trim, decreasing = TRUE, margin = "both"),
     n = 10,
     nf = 10)
```

4. Plot lexical dispersion

Plot shows the occurrences of the term 'python' and 'r' in across all documents for the state of Oregon (OR). The state was chosen to give a natural subset of all documents initially included.

The lexical dispersion appears to indicate the use of the terms 'python' and 'r' often occur in conjunction in the documents (job descriptions) which would indicate the listings are listing the two programming languages in or near the same sentence. 

```{r lexical-python-r-plot, fig.width=10,fig.height=11}

my_corpus_sub <- corpus_subset(my_corpus, state == "OR")

theme_set(theme_bw())

g <- textplot_xray(
     kwic(my_corpus_sub, pattern = "python"),
     kwic(my_corpus_sub, pattern = "r")
)

g + aes(color = keyword) + 
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = "none")

```

<br>
<div style="margin-bottom:25px;"></div>
## Bigrams

From the tutorial at **https://www.tidytextmining.com/**. The bigrams identify the word pairs that occur the most frequently

1. Identify bigrams of n=2.

This exercise finds bigrams of two words. The function does allow for bigrams of greater than two.

```{r identify-bigrams}
jobs_bigrams <- jobs_df %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2)

jobs_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- jobs_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# This result is valuable
bigram_counts %>% print(n = 20)
```

2. Filter the bigrams

Include bigrams that occurred at least 1250 times in order to filter out visual noise.

```{r filter-bigrams, fig.width=10,fig.height=11}
# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 1250) %>%
  graph_from_data_frame()

bigram_graph
```

3. Visualize the bigrams in Network plot

```{r bigrams-plot-part1, fig.width=10,fig.height=11}
set.seed(2020)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

4. Visualize the bigrams with Directional plot

```{r bigrams-plot-part2, fig.width=10,fig.height=11}
set.seed(2021)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

<br>
<div style="margin-bottom:25px;"></div>
## Frequency Plot

Simple bar plot of the most common words across all the job listings' descriptions.

```{r common-words-plot-flip, fig.width=10,fig.height=11}
jobs_words <- jobs_df %>%
  unnest_tokens(word, job_description) %>%
  anti_join(stop_words) %>%
  count(uniq_id, word, sort = TRUE)

total_words <- jobs_words %>% 
  group_by(uniq_id) %>% 
  summarize(total = sum(n))

jobs_words <- left_join(jobs_words, total_words)

jobs_words <- jobs_words %>%
    anti_join(stop_words)

jobs_words %>%
  count(word, sort = TRUE)

jobs_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 2500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


<br>
<div style="margin-bottom:25px;"></div>
## Words by Frequency  

```{r}

#Create a term document matrix based upon the variable descriptions
tdm <- TermDocumentMatrix(descriptions)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)

#Creates a frequency table for words
freqTable <- data.frame(word = names(v), freq=v)


```

<br>
Here is a frequency table and barplot for words found in job descriptions:  


```{r}
freqTable

#Bar plot of the 20 most common words
barplot(v[1:20], col = "#003366", las = 2)

```

<br>
<div style="margin-bottom:25px;"></div>
## Word Cloud  
<br>
*A Word Cloud Based Upon the Job Descriptions*^[Reference code was found here: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know]

```{r}
set.seed(1234)
wordcloud(words = freqTable$word, freq = freqTable$freq, min.freq = 100,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```



<br>
<div style="margin-bottom:50px;"></div>
# Analysis  

After mining the job descriptions of our data set, the team met again over Zoom to discuss the results of our Pollock method for looking at the data.  

Observations:  

1. The Equal Employment Opportunity (EEO) information for jobs was impacting the frequencies. 

    An example of an EEO is below:  

    CUNY encourages people with disabilities, minorities, veterans and women to apply. At CUNY, Italian Americans are also included among our protected groups. Applicants and employees will not be discriminated against on the basis of any legally protected category, including sexual orientation or gender identity. EEO/AA/Vet/Disability Employer.^[https://www.cuny.edu/about/administration/offices/hr/recruitment-diversity/includes/eeo-statement/]  
    
    Such words as religion, nation of origin, and diversity would appear in our frequencies. We would have to exclude these. We would not have been able to make this decision if we did not regroup.  
    
2. Loss of context  

    A word or phrase can be taken out of context. For example, the word *experience* sounds good by itself, but when it is counted it could have been within a sentence that asked for 3-5 years of *experience* or *no experience needed*. In addition, we did not examine the role or title of the job description. A senior data scientist would require a particular gamut of skills different from an entry level data scientist. One job title might require a PhD as opposed to a Bachelors degree. 
    
3. Hard Skills  

    After going through the analysis, these were the top 5 hard skills we determined by looking at the data together. Note: hard skills are "teachable abilities or skill sets that are easy to quantify."^[https://www.thebalancecareers.com/hard-skills-vs-soft-skills-2063780]  
    
    + Programming Skills: Python, R, SQL  
    + Platforms: Tableau, Apache Spark, Hadoop, AWS  
    + Computer Science  
    + Statistics, Mathematics, Algorithms  
    + Experience and Education  
    
4. Soft Skills  

    The following were the top 5 soft skills we determined by looking at the data together. Note: soft skills are "subjective skills that are much harder to quantify."^[https://www.thebalancecareers.com/hard-skills-vs-soft-skills-2063780]  
    
    + Communication Skills: presentation skills
    + Interpersonal Skills: i.e. EQ Emotional Intelligence
    + Team Player  
    + Analytical  
    + Motivated, driven  
    
5. Debugging?  

    From the number of days spent debugging and troubleshooting, the team saw what was absent was the skill of debugging. Why was troubleshooting seemingly absent from the job descriptions?
    
    
    
<br>
<div style="margin-bottom:50px;"></div>
# Conclusions  

The above skills are highly subjective as the team was looking at the data that was before us. From what the data and analysis provided, did not provide any earth-breaking "Aha" moments. We did make a note that the data we obtained is for job postings and represented only a slice of the skills needed to be a data scientist. Our analysis is not a holistic one.  

Every company will look for the best ideal candidates and the job posting is one step in any recruitment process. For every job, candidates have to interview and sometimes have to go through a barrage of interpersonal relationships with managers and headhunters.  

For future steps, it would be necessary to include headhunters and other recruiters to provide data. In addition, a breakdown of the skills by job title might produce additional insight. For the Azure text analytics, one can remove the EEO policies from the job descriptions and employ `RJson` to encapsulate the job descriptions as JSON objects to be analyzed.

