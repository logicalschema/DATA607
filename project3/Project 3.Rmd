---
title: "Data 607 Project 3"
author: "Sung Lee"
date: "3/21/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: lumen
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project3 "Data 607 Project 3 Rpubs")
<br>
[Rmd on GitHub](https://github.com/logicalschema/DATA607/blob/master/project3/Project%203.Rmd "Team MPSV Project 3 Assignment GitHub")

<div style="margin-bottom:50px;"></div>
# Introduction  




These are the libraries we will be using.
```{r loadlib, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(RMySQL)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(RWeka)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(knitr)


```


<div style="margin-bottom:50px;"></div>
# Approach

We had discussions about which sets of data to use. The internet is filled with a plethora of data, but few were of applicable use. Linkedin was not readily accessible. 


<div style="margin-bottom:50px;"></div>
# Importing

The data we ended up using is from the web site https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa. Phil was able to obtain the csv. The data was scraped using JobsPikr. The csv consists of 10,000 records and had the following as text fields: `crawl_timestamp`, `url`, `job_title`, `category`, `company_name`, `city`, `state`, `country`, `inferred_city`, `inferred_state`, `inferred_country`, `post_date`, `job_description`, `job_type`, `salary_offered`, `job_board`, `geo`, `cursor`, `contact_email`, `contact_phone_number`, `uniq_id`, and `html_job_desc`.  

This is the original csv with field names.

![CSV Fieldnames](https://github.com/logicalschema/DATA607/raw/master/project3/images/data_original_csv.png)

Inital import of the csv file.

```{r, warning=FALSE}

# Using read_csv as it allows for on the fly decompression of zip csv files
jobs <- read_csv("https://github.com/logicalschema/DATA607/raw/master/project3/data/data_scientist_united_states_job_postings_jobspikr.csv.gz")

# Look that the first group of imported rows
head(jobs)

```


Initial problems and solutions:  

1. The csv file is 48.5 MB and the character set had UTF-8 characters.  

    The file was zipped up to reduce download time. When the file is imported into R, we will tidy the data.

2. HTML code was left in the `job_description` field. Evidence of the HTML scrape of the data source.  

    Invalid characters will be removed.

3. Initial load of the csv into R would take time depending upon the hardware of the group member's system.  

    With the zip file and cutting up the file into smaller pieces, this would reduce the need for additional RAM.

4. How would we convert to a normalized database?  

    The csv was converted to a MySQL script file. The header column names were encapsulated with `"` marks. Workbench was used to create a new table on the MySQL database. Subsequently, through SQL SELECT, we normalized the data.
    
    In addition, when the data was imported in a database, Sung ran Text Analytics on the `job_description` column to find key phrases. This information was used to create a new column called `keyphrases` in the normalized database.

5. Some group member's machines did not have the same amount of RAM. Vanita's laptop had 8 GB RAM and Sung's laptop was running 16 GB.  

<br>
**How did the data look in Azure?**

When the data was imported into Microsoft's cloud service, we ran some initial tests on the data to look for patterns. Using Microsoft's Text Analytics^[https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-call-api] tools the following is a word cloud^[Note the image was created with stop words on https://github.com/logicalschema/DATA607/raw/master/project3/images/stopwords.jpg] of the keyphrases that were discovered by Microsoft's AI:

![Word Cloud Courtesy of Azure](https://github.com/logicalschema/DATA607/raw/master/project3/images/powerbi_wordcloud.png)

The key phrases were stored in the database. Because the use of Microsoft's service was limited, we found the need for persistent storage of the results. Sung had used up his $200 credit for one of his subscriptions within a week.


```{r}
mydb <- dbConnect(MySQL(), 
                  user='data607@data607', 
                  password='student#2020',
                  dbname='project3data607', 
                  host='data607.mysql.database.azure.com'
                  )

#keyphrases is stored in the meta table of the database
rs <- dbSendQuery(mydb, "SELECT keyphrases FROM meta")
dbRows <- dbFetch(rs)

#This the 
head(dbRows)
kable(dbRows, caption = "Sample Key Phrases")

```







<br>
<div style="margin-bottom 25px;"></div>
## Excursus {.tabset}

This is a brief description of some of the steps taken to help alleviate the problems of importing the information for use in our project.



<div style="margin-bottom 25px;"></div>
### Infrastructure

**Azure, MySQL, and Cognitive Text Analytics**
Sung wanted to experiment with cloud computing and so he used his SPS email to create a trial account for Azure. He created a MySQL database instance to host the databases used. Each trial account is given a $200 credit. 

**MySQL database access**
This connection is made public but is restricted by IP address. If you desire access, please email one of the Team members.

*Server*: data607.mysql.database.azure.com  
*Port*: 3306  
*Username*: data607@data607  
*Password*: student#2020  

Default timeout settings for the database server had to be lengthened to enable longer processing.

<br>

This is a [link](https://github.com/logicalschema/DATA607/raw/master/project3/data/project3data607.sql.gz) to the `mysqldump` of the database.  

***Diagram of Database***
![E-R Digram](https://raw.githubusercontent.com/logicalschema/DATA607/master/project3/images/database.png)




### Tools

**Microsoft's Cognitive Text Analytics, Power BI, Slack, GitHub, and MySQL Workbench**
In addition to the database instance, he created, Sung created an instance for Azure's Cognitive Text Analytics to experiment to see what information Microsoft's AI can mine from our data. The tools were used to facilitate handling the data.

Power BI is a Microsoft product that is used to visualize data. It was employed to work with Microsoft's Cognitive Text Analytics to extrapolate keyphrases from the `job_descriptions` of the data and to create a simple word cloud to compare with our later analysis.

Slack and GitHub were used to collaborate. Files were exchanged via Slack. Code was edited via GitHub.

MySQL Workbench was used to connect to the MySQL database.


### Data Management

As an option to curtail the amount of time needed to process the 10,000 row csv, group members worked with a small subset of the file, tested code, and then would work with the larger data set. Some group members machines would complain about memory errors. 

Files being worked on from Github were compressed to enable quicker transport across the network.

This is an example of a memory error on Sung's laptop when trying to text mine on the job description data:
![Memory Error](https://github.com/logicalschema/DATA607/raw/master/project3/images/memory_error.png)  




<div style="margin-bottom:50px;"></div>

# Tidying



<div style="margin-bottom:50px;"></div>
# Analysis


## Sections {.tabset}

### Section 1

### Section 2

### Section 3





# Conclusions

