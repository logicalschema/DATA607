---
title: "Data 607 Project 3"
author: "Sung Lee"
date: "3/21/2020"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
number_sections: true
theme: lumen
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project3 "Data 607 Project 3 Rpubs")
<br>
[Rmd on GitHub](https://github.com/logicalschema/DATA607/blob/master/project3/Project%203.Rmd "Team MPSV Project 3 Assignment GitHub")

<div style="margin-bottom:50px;"></div>
# Introduction



<div style="margin-bottom:50px;"></div>
# Tools



<div style="margin-bottom:50px;"></div>
# Approach

We had discussions about which sets of data to use. The internet is filled with a plethora of data, but few were of applicable use. Linkedin was not readily accessible. 


<div style="margin-bottom:50px;"></div>
# Importing

The data we ended up using is from the web site https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa. Phil was able to obtain the csv. The data was scraped using JobsPikr. The csv consists of 10,000 records and had the following as text fields: `crawl_timestamp`, `url`, `job_title`, `category`, `company_name`, `city`, `state`, `country`, `inferred_city`, `inferred_state`, `inferred_country`, `post_date`, `job_description`, `job_type`, `salary_offered`, `job_board`, `geo`, `cursor`, `contact_email`, `contact_phone_number`, `uniq_id`, and `html_job_desc`. 

Initial problems: 
1. The csv file is 48.5 MB and the character set had UTF-8 characters. 
2. HTML code was left in the `job_description` field.
3. Initial load of the csv into R would take time depending upon the hardware of the group member's system.  
4. How would we convert to a normalized database?
5. Some group member's machines did not have the same amount of RAM. Vanita's laptop had 8 GB RAM and Sung's laptop was running 16 GB.


<div style="margin-bottom 25px;"></div>
## Excursus {.tabset}

This is a brief description of some of the steps taken to help alleviate the problems of importing the information for use in our project.



<div style="margin-bottom 25px;"></div>
### Infrastructure

**Azure, MySQL, and Cognitive Text Analytics**
Sung wanted to experiment with cloud computing and so he used his SPS email to create a trial account for Azure. He created a MySQL database instance to host the databases used. Each trial account is given a $200 credit. 

**MySQL database access**
This connection is made public but is restricted by IP address. If you desire access, please email one of the Team members.

*Server*: data607.mysql.database.azure.com  
*Port*: 3306  
*Username*: data607@data607  
*Password*: student#2020  

<br>

This is a [link](https://github.com/logicalschema/DATA607/raw/master/project3/data/project3data607.sql.gz) to the `mysqldump` of the database.  

***Diagram of Database***
![E-R Digram](https://raw.githubusercontent.com/logicalschema/DATA607/master/project3/images/database.png)




### Tools

**Microsoft's Cognitive Text Analytics, Power BI, Slack, Github, and MySQL Workbench**
In addition to the database instance, he created, Sung created an instance for Azure's Cognitive Text Analytics to experiment to see what information Microsoft's AI can mine from our data. The tools were used to facilitate handling the data.

Power BI is a Microsoft product that is used to visualize data. It was employed to work with Microsoft's Cognitive Text Analytics to extrapolate keyphrases from the `job_descriptions` of the data and to create a simple word cloud to compare with our later analysis.

### Data Management

As an option to curtail the amount of time needed to process the 10,000 row csv, group members worked with a small subset of the file, tested code, and then would work with the larger data set. Some group members machines would complain about memory errors. 



<div style="margin-bottom:50px;"></div>
# Tidying



<div style="margin-bottom:50px;"></div>
# Analysis


## Sections {.tabset}

### Section 1

### Section 2

### Section 3





# Conclusions

