---
title: "Data 607 Project 3"
author: "Sung Lee"
date: "3/21/2020"
output: 
  html_document:
    code_folding: hide
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
number_sections: true
theme: lumen
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project3 "Data 607 Project 3 Rpubs")
<br>
[Rmd on GitHub](https://github.com/logicalschema/DATA607/blob/master/project3/Project%203.Rmd "Team MPSV Project 3 Assignment GitHub")


# Introduction




# Tools




# Approach

We had discussions about which sets of data to use. The internet is filled with a plethora of data, but few were of applicable use. Linkedin was not readily accessible. 

# Importing

The data we ended up using is from the web site https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa. Phil was able to obtain the csv. The data was scraped using JobsPikr. The csv consists of 10,000 records and had the following as text fields: `crawl_timestamp`, `url`, `job_title`, `category`, `company_name`, `city`, `state`, `country`, `inferred_city`, `inferred_state`, `inferred_country`, `post_date`, `job_description`, `job_type`, `salary_offered`, `job_board`, `geo`, `cursor`, `contact_email`, `contact_phone_number`, `uniq_id`, and `html_job_desc`. 

Initial problems: 
1. The csv file is 48.5 MB and the character set had UTF-8 characters. 
2. HTML code was left in the `job_description` field.
3. Initial load of the csv into R would take time depending upon the hardware of the group member's system.  
4. How would we convert to a normalized database?












# Analysis


## Sections {.tabset}

### Section 1

### Section 2

### Section 3





# Conclusions

