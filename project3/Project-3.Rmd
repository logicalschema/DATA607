---
title: "Data 607 Project 3"
author: "Matt Mecoli, Philip Tanofsky, Sung Lee, Vanita Thompson"
date: "3/21/2020"
output: 
  html_document:
    code_folding: show
    df_print: paged
    toc: true
    toc_float: true
    toc_collapsed: true
    smooth_scroll: false
    toc_depth: 3
number_sections: true
theme: lumen
---

[Assignment on RPubs](https://rpubs.com/logicalschema/data607_project3 "Data 607 Project 3 Rpubs")
<br>
[Rmd on GitHub](https://github.com/logicalschema/DATA607/blob/master/project3/Project%203.Rmd "Team MPSV Project 3 Assignment GitHub")

<div style="margin-bottom:50px;"></div>
# Introduction  

The purpose of this assignment is to answer the question, "Which are the most valued data science skills?" Within the team we have had experiences applying for jobs, but we were each relatively new to the data science vernacular. For example, we have one seasoned programmer and a student with a background in chemical engineering. This project is our endeavor in how we answered this question. 


<div style="margin-bottom:50px;"></div>
# Approach

We had discussions about which sets of data to use. The internet is filled with a plethora of data, but few were of applicable use. Linkedin was not readily accessible. We worked from our experiences with applying for jobs and during some brainstorming over Zoom, we came up with searching government sites for available data, Kaggle, and the possibility of web scraping Monster or Indeed. True, we did have our own presuppositions about needed skills for a data scientist, but we wanted the data to speak for itself and hopefully challenge our presuppositions.

**Jackson Pollock the Data**  

Our thinking was that if we obtain data science job listings, we would mine the text for words and phrases. In other words, if we splash the data in front of us, find some frequencies, perhaps a pattern would arise. 

![Jackson Pollock American Abstract Oil on Canvas](https://raw.githubusercontent.com/logicalschema/DATA607/master/project3/images/pollock.jpg){width=0.25}

<br>
After googling and searching, we settled on a data set that will be reviewed in the next section. However, little did we as a group realize how much cleaning and hours would be committed to having data that was accessible and usable for the entire team.  

Continuing on, the following are the R libraries we will be using.  

```{r loadlib, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(RMySQL)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(RWeka)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(knitr)
library(summarytools)

```

`RMySQL` is used for working with a MySQL database. `tm` and `RWeka` were used for text mining. Packages `SnowballC`, `wordcloud`, and `RColorBrewer` were used for creating the word cloud in R. `stringr`, `tidytext`, and `tidyverse` were used for text manipulations. `ggplot2` was used for graphs and `knitr` was used for making some tables. `summarytools` provides a handy frequency function to find tally the occurrences in a column.


<div style="margin-bottom:50px;"></div>
# Importing

The data we ended up using is from the web site https://data.world/jobspikr/10000-data-scientist-job-postings-from-the-usa. Phil was able to obtain the csv. The data was scraped using JobsPikr. The csv consists of 10,000 records and had the following as text fields: `crawl_timestamp`, `url`, `job_title`, `category`, `company_name`, `city`, `state`, `country`, `inferred_city`, `inferred_state`, `inferred_country`, `post_date`, `job_description`, `job_type`, `salary_offered`, `job_board`, `geo`, `cursor`, `contact_email`, `contact_phone_number`, `uniq_id`, and `html_job_desc`.  

This is the original csv with field names.

![CSV Fieldnames](https://github.com/logicalschema/DATA607/raw/master/project3/images/data_original_csv.png)

Inital import of the csv file.

```{r, warning=FALSE}

# Using read_csv as it allows for on the fly decompression of zip csv files
jobs <- read_csv("https://github.com/logicalschema/DATA607/raw/master/project3/data/data_scientist_united_states_job_postings_jobspikr.csv.gz")

# Look that the first group of imported rows
head(jobs)

```

```{r, echo=FALSE}

#To remove the large variable that was created
rm(jobs)

```


Initial problems and solutions:  

1. The csv file is 48.5 MB and the character set had UTF-8 characters.  

    The file was zipped up to reduce download time. When the file is imported into R, we will tidy the data.

2. HTML code was left in the `job_description` field. Evidence of the HTML scrape of the data source.  

    Invalid characters will be removed.

3. Initial load of the csv into R would take time depending upon the hardware of the group member's system.  

    With the zip file and cutting up the file into smaller pieces, this would reduce the need for additional RAM.

4. How would we convert to a normalized database?  

    The csv was converted to a MySQL script file. The header column names were encapsulated with `"` marks. Workbench was used to create a new table on the MySQL database. Subsequently, through SQL SELECT, we normalized the data.
    
    In addition, when the data was imported in a database, Sung ran Text Analytics on the `job_description` column to find key phrases. This information was used to create a new column called `keyphrases` in the normalized database.

5. Some group member's machines did not have the same amount of RAM. Vanita's laptop had 8 GB RAM and Sung's laptop was running 16 GB.  

<br>
**How did the data look in Azure?**

When the data was imported into Microsoft's cloud service, we ran some initial tests on the data to look for patterns. Using Microsoft's Text Analytics^[https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-call-api] tools the following is a word cloud^[Note the image was created with stop words on Power Bi https://github.com/logicalschema/DATA607/raw/master/project3/images/stopwords.jpg] of the keyphrases that were discovered by Microsoft's AI:

![Word Cloud Courtesy of Azure](https://github.com/logicalschema/DATA607/raw/master/project3/images/powerbi_wordcloud.png)

The key phrases were stored in the database. Key phrases were extrapolated from the job descriptions for each of the jobs listed. Because the use of Microsoft's service was limited, we found the need for persistent storage of the results. Sung had used up his $200 credit for one of his subscriptions within a week of testing. In working with the Azure Text Analytics API, there is an option to send data via JSON to have it analyzed.  



<br>
<div style="margin-bottom 25px;"></div>
## Excursus {.tabset}

This is a brief description of some of the steps taken to help alleviate the problems of importing the information for use in our project.



<div style="margin-bottom 25px;"></div>
### Infrastructure

**Azure, MySQL, and Cognitive Text Analytics**
Sung wanted to experiment with cloud computing and so he used his SPS email to create a trial account for Azure. He created a MySQL database instance to host the databases used. Each trial account is given a $200 credit. 

**MySQL database access**
This connection is made public but is restricted by IP address. If you desire access, please email one of the Team members.

*Server*: data607.mysql.database.azure.com  
*Port*: 3306  
*Username*: data607@data607  
*Password*: student#2020  

Default timeout settings for the database server had to be lengthened to enable longer processing.

<br>

This is a [link](https://github.com/logicalschema/DATA607/raw/master/project3/data/project3data607.sql.gz) to the `mysqldump` of the database.  

***Diagram of Database***
![E-R Digram](https://raw.githubusercontent.com/logicalschema/DATA607/master/project3/images/database.png)




### Tools

**Microsoft's Cognitive Text Analytics, Power BI, Slack, GitHub, and MySQL Workbench**  

In addition to the database instance, he created, Sung created an instance for Azure's Cognitive Text Analytics to experiment to see what information Microsoft's AI can mine from our data. The tools were used to facilitate handling the data.

Power BI is a Microsoft product that is used to visualize data. It was employed to work with Microsoft's Cognitive Text Analytics to extrapolate keyphrases from the `job_descriptions` of the data and to create a simple word cloud to compare with our later analysis.

Slack and GitHub were used to collaborate. Files were exchanged via Slack. Code was edited via GitHub.

MySQL Workbench was used to connect to the MySQL database.


### Data Management

As an option to curtail the amount of time needed to process the 10,000 row csv, group members worked with a small subset of the file, tested code, and then would work with the larger data set. Some group members machines would complain about memory errors. 

Files being worked on from Github were compressed to enable quicker transport across the network.  

This is an example of a memory error on Sung's laptop when trying to text mine on the job description data:
![Memory Error](https://github.com/logicalschema/DATA607/raw/master/project3/images/memory_error.png)  

To avert this problem, Sung modified his code to use the `Corpus` object instead of `VCorpus` for collections of text documents. A `VCorpus` is described as a *volatile* corpus that is "fully kept in memory".^[https://www.rdocumentation.org/packages/tm/versions/0.7-7/topics/VCorpus] 

In addition, wherever possible, snippets of the data objects were viewed and knitted in R Markdown. So instead of viewing 10,000 rows, we did our best to display 10-20 rows of a data set.  


<div style="margin-bottom:50px;"></div>
# Tidying 

Since we have imported our data into a database, we still need to clean up the job descriptions. We hope that we will be able to tidy the data such that we would be able to determine frequencies and see what picture the data paints for us.

We will now begin to query the database for the job descriptions and the key phrases that Azure extrapolated for us. We will store job descriptions in the variable `descriptions` and the key phrases in `keyphrases`. 


```{r}
#Form a connection to the MySQL database
mydb <- dbConnect(MySQL(), 
                  user='data607@data607', 
                  password='student#2020',
                  dbname='project3data607', 
                  host='data607.mysql.database.azure.com'
                  )

#Key phrases is stored in the meta table of the database
rs <- dbSendQuery(mydb, "SELECT keyphrases FROM meta")

#Remove argument for n = -1 if you don't want all the records in the db
dbRows <- dbFetch(rs, n = -1 )
#dbRows <- dbFetch(rs)
dbClearResult(rs)

#Job descriptions are stored in the job table of the database
rs <- dbSendQuery(mydb, "SELECT job_description FROM job")

#Remove argument for n = -1 if you don't want all the records in the db
descriptions <- dbFetch(rs, n = -1)
#descriptions <- dbFetch(rs)
dbClearResult(rs)

#This a sample of the Key Phrases from Azure
kable(dbRows[1:5, ], 
      caption = "Job Description Key Phrases", 
      col.names = c("Key Phrases") 
      )

```

<br>
*Azure Key Phrases*  

Let's get the unique key phrases that Azure found.  


```{r, warning=FALSE}

#Grab the unique keyphrases after some clean up: 
keyphrases <- unlist(str_split(dbRows, ","))
keyphrases <- str_replace_all(keyphrases, "^ ", "")
keyphrases <- str_replace_all(keyphrases, "[^\x20-\x7E]", "")
keyphrases <- str_replace_all(keyphrases, "\"", "")

# Remove the "c(" in the first element
keyphrases[[1]] <- keyphrases[[1]] %>% 
  str_replace("c[(]", "")

keyphrases <- as.data.frame(keyphrases)

#Some of the key phrases
keyphrases



#Frequency chart of the key phrases using the summary tools package
freq(keyphrases$keyphrases, order = "freq", style = "rmarkdown", rows = 1:20)


```

Briefly looking at the frequencies found in the key phrases, some skills started to appear such as Python (count: 6471), Computer Science (4441), and SQL (2672). However, some problems came us as the boiler plate for many job descriptions about diversity and Equal opportunity policies were appearing as evident by the frequency of such phrases as Religion (2746), Color (2675), and National Origin (2649).


<br>
*Tidy the job descriptions*  

Before we continue, we have to tidy the job descriptions. We will remove non-ASCII characters, remove some unecessary characters, stop words, make the text lowercase, and white space. Stop words are common words such as *the*, *is*, *at*, *which*, etc. This will be handy in our analysis.

```{r, warning=FALSE}


#Remove non-ASCII characters
descriptions <-  gsub("[^\x20-\x7E]", " ", descriptions)


#Convert our variable to a Corpus variable for the package tm to work on.
#A corpus is a collection of documents
descriptions <- Corpus(VectorSource(descriptions))

#Create a function to make patterns into spaces
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))

descriptions <- tm_map(descriptions, toSpace, "/")
descriptions <- tm_map(descriptions, toSpace, "\n")
descriptions <- tm_map(descriptions, toSpace, "@")
descriptions <- tm_map(descriptions, toSpace, "\\|")

#Convert the text to lowercase
descriptions <- tm_map(descriptions, content_transformer(tolower))

#Remove English stop words
descriptions <- tm_map(descriptions, removeWords, stopwords("english"))

#Remove punctuation , ., etc.
descriptions <- tm_map(descriptions, removePunctuation)

#Remove whitespace
descriptions <- tm_map(descriptions, stripWhitespace)

```






<div style="margin-bottom:50px;"></div>
# Text Mining {.tabset}


After we have imported and tidied our data, we will perform tests to mine it.


<br>
<div style="margin-bottom:25px;"></div>
## Words by Frequency  

```{r}

#Create a term document matrix based upon the variable descriptions
tdm <- TermDocumentMatrix(descriptions)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)

#Creates a frequency table for words
freqTable <- data.frame(word = names(v), freq=v)


```

<br>
Here is a frequency table and barplot for words found in job descriptions:  


```{r}
freqTable

#Bar plot of the 20 most common words
barplot(v[1:20], col = "#003366", las = 2)

```

<br>
<div style="margin-bottom:25px;"></div>
## Word Cloud  
<br>
*A Word Cloud Based Upon the Job Descriptions*^[Reference code was found here: http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know]

```{r}
set.seed(1234)
wordcloud(words = freqTable$word, freq = freqTable$freq, min.freq = 100,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```



<br>
<div style="margin-bottom:50px;"></div>
# Analysis  

After mining the job descriptions of our data set, the team met again over Zoom to discuss the results of our Pollock method for looking at the data.  

Observations:  

1. The Equal Employment Opportunity (EEO) information for jobs was impacting the frequencies. 

    An example of an EEO is below:  

    CUNY encourages people with disabilities, minorities, veterans and women to apply. At CUNY, Italian Americans are also included among our protected groups. Applicants and employees will not be discriminated against on the basis of any legally protected category, including sexual orientation or gender identity. EEO/AA/Vet/Disability Employer.^[https://www.cuny.edu/about/administration/offices/hr/recruitment-diversity/includes/eeo-statement/]  
    
    Such words as religion, nation of origin, and diversity would appear in our frequencies. We would have to exclude these. We would not have been able to make this decision if we did not regroup.  
    
2. Loss of context  

    A word or phrase can be taken out of context. For example, the word *experience* sounds good by itself, but when it is counted it could have been within a sentence that asked for 3-5 years of *experience* or *no experience needed*. In addition, we did not examine the role or title of the job description. A senior data scientist would require a particular gamut of skills different from an entry level data scientist. One job title might require a PhD as opposed to a Bachelors degree. 
    
3. Hard Skills  

    After going through the analysis, these were the top 5 hard skills we determined by looking at the data together. Note: hard skills are "teachable abilities or skill sets that are easy to quantify."^[https://www.thebalancecareers.com/hard-skills-vs-soft-skills-2063780]  
    
    + Programming Skills: Python, R, SQL  
    + Platforms: Tableau, Apache Spark, Hadoop, AWS  
    + Computer Science  
    + Statistics, Mathematics, Algorithms  
    + Experience and Education  
    
4. Soft Skills  

    The following were the top 5 soft skills we determined by looking at the data together. Note: soft skills are "subjective skills that are much harder to quantify."^[https://www.thebalancecareers.com/hard-skills-vs-soft-skills-2063780]  
    
    + Communication Skills: presentation skills
    + Interpersonal Skills: i.e. EQ Emotional Intelligence
    + Team Player  
    + Analytical  
    + Motivated, driven  
    
5. Debugging?  

    From the number of days spent debugging and troubleshooting, the team saw what was absent was the skill of debugging. Why was troubleshooting seemingly absent from the job descriptions?
    
    
    
<br>
<div style="margin-bottom:50px;"></div>
# Conclusions  

The above skills are highly subjective as the team was looking at the data that was before us. From what the data and analysis provided, did not provide any earth-breaking "Aha" moments. We did make a note that the data we obtained is for job postings and represented only a slice of the skills needed to be a data scientist. Our analysis is not a holistic one.  

Every company will look for the best ideal candidates and the job posting is one step in any recruitment process. For every job, candidates have to interview and sometimes have to go through a barrage of interpersonal relationships with managers and headhunters.  

For future steps, it would be necessary to include headhunters and other recruiters to provide data. In addition, a breakdown of the skills by job title might produce additional insight. For the Azure text analytics, one can remove the EEO policies from the job descriptions and employ `RJson` to encapsulate the job descriptions as JSON objects to be analyzed.















